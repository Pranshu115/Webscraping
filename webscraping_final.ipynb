{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import re\n"
      ],
      "metadata": {
        "id": "Y1Mt24R1s9l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base URL\n",
        "base_url = \"https://www.indcareer.com\"\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans and extracts readable text from HTML content.\"\"\"\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text(separator=\" \").strip().replace('[email\\xa0protected]', '')\n",
        "\n",
        "def extract_email(text):\n",
        "    \"\"\"Extracts email addresses from text.\"\"\"\n",
        "    email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    return \", \".join(emails) if emails else \"\"\n",
        "\n",
        "def extract_phone_numbers(text):\n",
        "    \"\"\"Extracts phone numbers from text.\"\"\"\n",
        "    phone_pattern = r\"\\+?\\d{1,3}[-./\\s]?\\(?\\d{2,5}\\)?[-./\\s]?\\d{2,5}[-./\\s]?\\d{2,5}[-./\\s]?\\d{1,5}\"\n",
        "    phone_numbers = re.findall(phone_pattern, text)\n",
        "\n",
        "    valid_numbers = []\n",
        "    for phone in phone_numbers:\n",
        "        phone = re.sub(r\"[^\\d+]\", \"\", phone)\n",
        "        if phone.startswith(\"0\"):\n",
        "            phone = phone[1:]\n",
        "        if 10 <= len(phone) <= 12:\n",
        "            valid_numbers.append(f\"+91{phone}\" if not phone.startswith(\"+\") else phone)\n",
        "\n",
        "    return \", \".join(valid_numbers) if valid_numbers else \"\"\n"
      ],
      "metadata": {
        "id": "LKNxTisrtG-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of states and UTs formatted for URLs\n",
        "states_ut = [\n",
        "    \"andhra-pradesh\", \"arunachal-pradesh\", \"assam\", \"bihar\",\n",
        "    \"chhattisgarh\", \"goa\", \"gujarat\", \"haryana\", \"himachal-pradesh\", \"jharkhand\", \"karnataka\",\n",
        "    \"kerala\", \"madhya-pradesh\", \"maharashtra\", \"manipur\", \"meghalaya\", \"mizoram\", \"nagaland\",\n",
        "    \"odisha\", \"punjab\", \"rajasthan\", \"sikkim\", \"tamil-nadu\", \"telangana\", \"tripura\",\n",
        "    \"uttar-pradesh\", \"uttarakhand\", \"west-bengal\", \"andaman-nicobar-islands\", \"chandigarh\",\n",
        "    \"dadra-nagar-haveli-daman-diu\", \"delhi\", \"lakshadweep\", \"puducherry\", \"ladakh\", \"jammu-kashmir\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "Fj1sr3iqtG8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_college_links(state):\n",
        "    \"\"\"Fetches all college links for a given state.\"\"\"\n",
        "    print(f\"Scraping colleges in {state.title()}...\")\n",
        "\n",
        "    links = []\n",
        "    for x in tqdm(range(0, 120)):  # Adjust range if required\n",
        "        response = requests.get(f\"https://www.indcareer.com/find/all-colleges-in-{state}?page={x}\")\n",
        "        if response.status_code != 200:\n",
        "            break  # Stop if the page does not exist\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        h4s = soup.find_all(\"h4\")\n",
        "\n",
        "        for h4 in h4s:\n",
        "            a_tag = h4.find(\"a\")\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                links.append(base_url + a_tag['href'])\n",
        "\n",
        "    return links\n"
      ],
      "metadata": {
        "id": "M6cr-2kxtG5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_college_details(college_url, state):\n",
        "    \"\"\"Extracts details from a college page.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(college_url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        college_info = {\n",
        "            \"College Name\": soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"\",\n",
        "            \"State\": state.replace(\"-\", \" \").title()\n",
        "        }\n",
        "\n",
        "        email, phone, website, city, affiliation = \"\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        table = soup.find(\"table\")\n",
        "        if table:\n",
        "            rows = table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                columns = row.find_all([\"th\", \"td\"])\n",
        "                if len(columns) == 2:\n",
        "                    key = columns[0].get_text(strip=True)\n",
        "                    value = clean_text(columns[1].get_text(strip=True))\n",
        "\n",
        "                    if \"City\" in key:\n",
        "                        city = value\n",
        "                    if \"Affiliated to\" in key:\n",
        "                        affiliation = value\n",
        "                    if \"Email\" in key and \"@\" in value:\n",
        "                        email = extract_email(value)\n",
        "                    if \"Phone\" in key or \"Contact\" in key:\n",
        "                        phone = extract_phone_numbers(value)\n",
        "                    if \"Website\" in key:\n",
        "                        website_link = columns[1].find(\"a\", href=True)\n",
        "                        if website_link:\n",
        "                            website = website_link[\"href\"].strip()\n",
        "\n",
        "        full_text = soup.get_text()\n",
        "        if not email:\n",
        "            email = extract_email(full_text)\n",
        "        if not phone:\n",
        "            phone = extract_phone_numbers(full_text)\n",
        "\n",
        "        college_info[\"City\"] = city\n",
        "        college_info[\"Affiliated to\"] = affiliation\n",
        "        college_info[\"Email\"] = email if email else \"\"\n",
        "        college_info[\"Phone\"] = phone if phone else \"\"\n",
        "        college_info[\"Website\"] = website\n",
        "\n",
        "        return college_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving information for {college_url}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "kHrNZ2ZJtG2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_state_colleges(state):\n",
        "    \"\"\"Scrapes all colleges for a given state and returns a list of dictionaries.\"\"\"\n",
        "    college_info_list = []\n",
        "    college_links = get_college_links(state)\n",
        "\n",
        "    for college_url in tqdm(college_links):\n",
        "        college_info = get_college_details(college_url, state)\n",
        "        if college_info:\n",
        "            college_info_list.append(college_info)\n",
        "\n",
        "    return college_info_list\n"
      ],
      "metadata": {
        "id": "Abik-7r6tG0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As we have scraped the information of by taking two states at a time or five states at a time"
      ],
      "metadata": {
        "id": "nwAcHQ72uRG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_to_scrape = \"karnataka\"  # Change this to scrape a different state\n",
        "\n",
        "college_data = scrape_state_colleges(state_to_scrape)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(college_data)\n",
        "\n",
        "# Keep only required columns\n",
        "columns_to_keep = ['College Name', 'City', 'State', 'Affiliated to', 'Phone', 'Email', 'Website']\n",
        "df = df.reindex(columns=columns_to_keep, fill_value=\"\")\n",
        "\n",
        "# Save to Excel\n",
        "df.to_excel(f\"{state_to_scrape}_College_Information.xlsx\", index=False)\n",
        "\n",
        "print(f\"Data for {state_to_scrape} saved successfully!\")\n"
      ],
      "metadata": {
        "id": "CJcVjyd-tGyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_college_info = []\n",
        "for state in states_ut:\n",
        "    all_college_info.extend(scrape_state_colleges(state))\n",
        "\n",
        "df = pd.DataFrame(all_college_info)\n",
        "df.to_excel(\"All_States_College_Information.xlsx\", index=False)\n",
        "\n",
        "print(\"Scraping for all states completed!\")\n"
      ],
      "metadata": {
        "id": "EGpC1loitx8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqAe498SaVUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ea1a90-482a-48e0-85db-27510c25fabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined 7 files into combined_output.xlsx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def combine_xlsx_to_xlsx(file_paths, output_xlsx):\n",
        "    with pd.ExcelWriter(output_xlsx, engine='openpyxl') as writer:\n",
        "        for file in file_paths:\n",
        "            df = pd.read_excel(file, engine='openpyxl')\n",
        "            sheet_name = os.path.splitext(os.path.basename(file))[0][:31]\n",
        "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "    print(f\"Combined {len(file_paths)} files into {output_xlsx}\")\n",
        "\n",
        "\n",
        "file_paths = [\n",
        "    \"/content/Andaman...dadarnagar.xlsx\",\n",
        "    \"/content/Andhrapradesh_Arunachalpradesh.xlsx\",\n",
        "    \"/content/Assam_Bihar.xlsx\",\n",
        "    \"/content/Chhattishgarh_goa.xlsx\",\n",
        "    \"/content/Delhi....Jammu.xlsx\",\n",
        "    \"/content/Gujarat.....Nagaland.xlsx\",\n",
        "    \"/content/Odisha...WestBengal.xlsx\"\n",
        "]\n",
        "output_xlsx = \"combined_output.xlsx\"\n",
        "combine_xlsx_to_xlsx(file_paths, output_xlsx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFeHapExaVFa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}